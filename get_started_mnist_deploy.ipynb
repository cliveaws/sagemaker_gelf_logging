{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a Trained TensorFlow V2 Model\n",
    "\n",
    "In this notebook, we walk through the process of deploying a trained model to a SageMaker endpoint. If you recently ran [the notebook for training](get_started_mnist_deploy.ipynb) with %store% magic, the `model_data` can be restored. Otherwise, we retrieve the \n",
    "model artifact from a public S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setups\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "from sagemaker import get_execution_role, Session\n",
    "import boto3\n",
    "\n",
    "# Get global config\n",
    "with open(\"code/config.json\", \"r\") as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "sess = Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "%store -r tf_mnist_model_data\n",
    "\n",
    "\n",
    "try:\n",
    "    tf_mnist_model_data\n",
    "except NameError:\n",
    "    import json\n",
    "\n",
    "    # copy a pretrained model from a public bucket to your default bucket\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    bucket = CONFIG[\"public_bucket\"]\n",
    "    key = \"datasets/image/MNIST/model/tensorflow-training-2020-11-20-23-57-13-077/model.tar.gz\"\n",
    "    s3.download_file(bucket, key, \"model.tar.gz\")\n",
    "    tf_mnist_model_data = sess.upload_data(\n",
    "        path=\"model.tar.gz\", bucket=sess.default_bucket(), key_prefix=\"model/tensorflow\"\n",
    "    )\n",
    "    os.remove(\"model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-eu-west-1-693516733736/DEMO-tensorflow/mnist/tensorflow-training-2022-11-28-16-49-58-388/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(tf_mnist_model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login and pull the base container "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "2.10.0-cpu-py39-ubuntu20.04-sagemaker: Pulling from tensorflow-inference\n",
      "Digest: sha256:a9d0cb0458dc50daf3c7a86b0070ffc79fb8b26ad1a33c16c00e767da2bfd2e2\n",
      "Status: Image is up to date for 763104351884.dkr.ecr.eu-west-1.amazonaws.com/tensorflow-inference:2.10.0-cpu-py39-ubuntu20.04-sagemaker\n",
      "763104351884.dkr.ecr.eu-west-1.amazonaws.com/tensorflow-inference:2.10.0-cpu-py39-ubuntu20.04-sagemaker\n"
     ]
    }
   ],
   "source": [
    "!aws ecr get-login-password --region eu-west-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.eu-west-1.amazonaws.com\n",
    "!docker pull 763104351884.dkr.ecr.eu-west-1.amazonaws.com/tensorflow-inference:2.10.0-cpu-py39-ubuntu20.04-sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working in region eu-west-1\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  101.9kB\n",
      "Step 1/8 : FROM 763104351884.dkr.ecr.eu-west-1.amazonaws.com/tensorflow-inference:2.10.0-cpu-py39-ubuntu20.04-sagemaker\n",
      " ---> 4a0b5aebd088\n",
      "Step 2/8 : RUN git clone https://github.com/jerome-pouiller/reredirect.git\n",
      " ---> Using cache\n",
      " ---> f65494aed890\n",
      "Step 3/8 : RUN cd reredirect; make install\n",
      " ---> Using cache\n",
      " ---> 8a8c58a30231\n",
      "Step 4/8 : COPY python_service.py ./sagemaker/python_service.py\n",
      " ---> 2c6218637328\n",
      "Step 5/8 : COPY gelf_client.py /usr/bin/gelf_client.py\n",
      " ---> f0c9a9428b40\n",
      "Step 6/8 : COPY serve* ./sagemaker/\n",
      " ---> 8c3b71575fa2\n",
      "Step 7/8 : COPY gelf_client.py ./sagemaker/\n",
      " ---> eabc2be8a140\n",
      "Step 8/8 : RUN pip install pygelf\n",
      " ---> Running in a940dd2eeeba\n",
      "Collecting pygelf\n",
      "  Downloading pygelf-0.4.2-py3-none-any.whl (8.7 kB)\n",
      "Installing collected packages: pygelf\n",
      "Successfully installed pygelf-0.4.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container a940dd2eeeba\n",
      " ---> 753f3fdb61ad\n",
      "Successfully built 753f3fdb61ad\n",
      "Successfully tagged tfs-custom-attributes:latest\n",
      "The push refers to repository [693516733736.dkr.ecr.eu-west-1.amazonaws.com/tfs-custom-attributes]\n",
      "\n",
      "\u001b[1Baa4b29a7: Preparing \n",
      "\u001b[1Bd1e70118: Preparing \n",
      "\u001b[1Bdf8ac2a7: Preparing \n",
      "\u001b[1B637def30: Preparing \n",
      "\u001b[1B38ab91b5: Preparing \n",
      "\u001b[1Ba364fe8a: Preparing \n",
      "\u001b[1B5d2cadd3: Preparing \n",
      "\u001b[1B720301c6: Preparing \n",
      "\u001b[1Bb9481e7f: Preparing \n",
      "\u001b[1Bd0e70abc: Preparing \n",
      "\u001b[1B273efeb3: Preparing \n",
      "\u001b[1B7032160b: Preparing \n",
      "\u001b[1B155ae5d8: Preparing \n",
      "\u001b[1B1bf515b2: Preparing \n",
      "\u001b[1Bf856c6a4: Preparing \n",
      "\u001b[1B24f9f284: Preparing \n",
      "\u001b[1B1164e264: Preparing \n",
      "\u001b[1B5fc7fd47: Preparing \n",
      "\u001b[1B7b628bb3: Preparing \n",
      "\u001b[1Bf3b49ea4: Preparing \n",
      "\u001b[1B55f34fde: Preparing \n",
      "\u001b[1Bb0b414d2: Preparing \n",
      "\u001b[1B8c1cecdc: Preparing \n",
      "\u001b[1Bc27fd1ca: Preparing \n",
      "\u001b[2Bc27fd1ca: Layer already exists 9kB\u001b[21A\u001b[2K\u001b[25A\u001b[2K\u001b[17A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2Klatest: digest: sha256:04eca6486e7f69ba83b3d1d5e6efa6db0ec060873da110d88fb28d827bedb88d size: 5551\n"
     ]
    }
   ],
   "source": [
    "!cd Docker; ./build_and_push.sh tfs-custom-attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Model Object\n",
    "\n",
    "The `TensorFlowModel` class allows you to define an environment for making inference using your\n",
    "model artifact. Like `TensorFlow` estimator class we discussed \n",
    "[in this notebook for training an Tensorflow model](\n",
    "get_started_mnist_train.ipynb), it is high level API used to set up a docker image for your model hosting service.\n",
    "\n",
    "Once it is properly configured, it can be used to create a SageMaker\n",
    "endpoint on an EC2 instance. The SageMaker endpoint is a containerized environment that uses your trained model \n",
    "to make inference on incoming data via RESTful API calls. \n",
    "\n",
    "Some common parameters used to initiate the `TensorFlowModel` class are:\n",
    "- role: An IAM role to make AWS service requests\n",
    "- model_data: the S3 bucket URI of the compressed model artifact. It can be a path to a local file if the endpoint \n",
    "is to be deployed on the SageMaker instance you are using to run this notebook (local mode)\n",
    "- framework_version: version of the MXNet package to be used\n",
    "- py_version: python version to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TensorFlowModel(\n",
    "    role=role,\n",
    "    model_data=tf_mnist_model_data,\n",
    "    image_uri='693516733736.dkr.ecr.eu-west-1.amazonaws.com/tfs-custom-attributes',\n",
    "#    framework_version=\"2.3.1\",\n",
    "    entry_point='inference.py',\n",
    "    source_dir='code',\n",
    "    env={\n",
    "        'GELF_LOGGING_HOST': 'ec2-3-252-221-180.eu-west-1.compute.amazonaws.com',\n",
    "        'SAGEMAKER_GUNICORN_LOGLEVEL': 'debug',\n",
    "        'SAGEMAKER_TFS_NGINX_LOGLEVEL': 'info',\n",
    "    },\n",
    "\n",
    "#    vpc_config={\n",
    "#        'Subnets':['subnet-06765fb34548113ac', 'subnet-03a6e46aab4f22327', 'subnet-098d17333b6c28cb8'],\n",
    "#        'SecurityGroupIds': ['sg-007466a17b1a769af']\n",
    "#    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Inference Container\n",
    "Once the `TensorFlowModel` class is initiated, we can call its `deploy` method to run the container for the hosting\n",
    "service. Some common parameters needed to call `deploy` methods are:\n",
    "\n",
    "- initial_instance_count: the number of SageMaker instances to be used to run the hosting service.\n",
    "- instance_type: the type of SageMaker instance to run the hosting service. Set it to `local` if you want run the hosting service on the local SageMaker instance. Local mode are typically used for debugging. \n",
    "\n",
    "<span style=\"color:red\"> Note: local mode is not supported in SageMaker Studio </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# set local_mode to False if you want to deploy on a remote\n",
    "# SageMaker instance\n",
    "\n",
    "local_mode = False\n",
    "\n",
    "if local_mode:\n",
    "    instance_type = \"local\"\n",
    "else:\n",
    "    instance_type = \"ml.c4.xlarge\"\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions Against a SageMaker endpoint\n",
    "\n",
    "Once you have the `Predictor` instance returned by `model.deploy(...)`, you can send prediction requests to your endpoints. In this case, the model accepts normalized \n",
    "batch images in depth-minor convention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [[-0.501469672, 0.125417814, 3.46883965, 4.64842224, -2.77617955, 4.28768253, 2.66589975, 1.36938596, -0.058537744, -1.61590505], [-0.327813566, -0.30102843, 3.43387985, 4.47641134, -2.89153743, 4.08714199, 3.40422988, 1.24493074, -0.0500440709, -1.47943234], [0.0269407015, 0.166276217, 3.56435442, 4.27608204, -2.61176229, 3.91972399, 3.20264244, 0.915147364, -0.03201776, -1.71719718], [-0.21841307, -0.106868856, 3.63616967, 4.83664799, -2.72297335, 3.99172616, 2.37071943, 1.33915544, 0.0555677079, -1.77135897]]}\n"
     ]
    }
   ],
   "source": [
    "# use some dummy inputs\n",
    "import numpy as np\n",
    "\n",
    "dummy_inputs = {\"instances\": np.random.rand(4, 28, 28, 1).tolist()}\n",
    "args = {'CustomAttributes' : json.dumps({'content':'this is a test'})}\n",
    "\n",
    "res = predictor.predict(dummy_inputs, args)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm_rt = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'a40ce481-2aed-40b2-96e6-00afe1c547a7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'a40ce481-2aed-40b2-96e6-00afe1c547a7', 'x-amzn-sagemaker-custom-attributes': '{\"content\": \"This is some new content\", \"another thing\": \"this is another thing\", \"new_field\": \"this is a new field\"}', 'x-amzn-invoked-production-variant': 'AllTraffic', 'date': 'Wed, 14 Dec 2022 15:37:16 GMT', 'content-type': 'application/json', 'content-length': '541'}, 'RetryAttempts': 0}, 'ContentType': 'application/json', 'InvokedProductionVariant': 'AllTraffic', 'CustomAttributes': '{\"content\": \"This is some new content\", \"another thing\": \"this is another thing\", \"new_field\": \"this is a new field\"}', 'Body': <botocore.response.StreamingBody object at 0x7f0c1ce33280>}\n",
      "b'{\\n    \"predictions\": [[-0.251579493, -0.390359879, 2.69170284, 3.87544441, -2.74001908, 4.27564144, 3.27751398, 1.86887753, -0.037007086, -1.16503274], [-0.505161047, -0.117799483, 2.95479488, 4.40742874, -2.4230957, 4.22497225, 2.81765866, 1.23703635, 0.0723230168, -1.21113944], [-0.0923229903, -0.162988544, 3.31573653, 4.0395875, -2.73588443, 4.83563852, 3.80887914, 1.11997032, 0.173118532, -1.31682217], [-0.159318, 0.116288595, 3.45301461, 4.45775938, -2.4978528, 4.24132776, 3.24941039, 1.14289796, 0.0405410826, -1.57036352]\\n    ]\\n}'\n"
     ]
    }
   ],
   "source": [
    "dummy_inputs = {\"instances\": np.random.rand(4, 28, 28, 1).tolist()}\n",
    "\n",
    "res = sm_rt.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint_name,\n",
    "    Body=json.dumps(dummy_inputs),\n",
    "    ContentType='application/json',\n",
    "    Accept='application/json',\n",
    "    CustomAttributes=json.dumps({'content':'this is a test', 'another thing': 'this is another thing'}),\n",
    "#    TargetModel='string',\n",
    "#    TargetVariant='string',\n",
    "#    TargetContainerHostname='string',\n",
    "    InferenceId='testid',\n",
    "#    EnableExplanations='string'\n",
    ")\n",
    "\n",
    "print(res)\n",
    "#print(res['CustomAttributes'])\n",
    "data = res['Body'].read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "endpoint_url='https://runtime.sagemaker.eu-west-1.amazonaws.com/endpoints/tfs-custom-attributes-2022-11-30-14-30-49-972/invocations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [403]>\n"
     ]
    }
   ],
   "source": [
    "res = requests.post(endpoint_url, data={'key':'value'})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formats of the input and output data correspond directly to the request and response\n",
    "format of the `Predict` method in [TensorFlow Serving REST API](https://www.tensorflow.org/tfx/serving/api_rest), for example, the key of the array to be \n",
    "parsed to the model in the `dummy_inputs` needs to be called `instances`. Moreover, the input data needs to have a batch dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to see an example that cannot be processed by the endpoint\n",
    "\n",
    "# dummy_data = {\n",
    "#    'instances': np.random.rand(28, 28, 1).tolist()\n",
    "# }\n",
    "# print(predictor.predict(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use real MNIST test to test the endpoint. We use helper functions defined in `code.utils` to \n",
    "download MNIST data set and normalize the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mnist import mnist_to_numpy, normalize\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_dir = \"/tmp/data\"\n",
    "X, _ = mnist_to_numpy(data_dir, train=False)\n",
    "\n",
    "# randomly sample 16 images to inspect\n",
    "mask = random.sample(range(X.shape[0]), 16)\n",
    "samples = X[mask]\n",
    "\n",
    "# plot the images\n",
    "fig, axs = plt.subplots(nrows=1, ncols=16, figsize=(16, 1))\n",
    "\n",
    "for i, splt in enumerate(axs):\n",
    "    splt.imshow(samples[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model accepts normalized input, you will need to normalize the samples before \n",
    "sending it to the endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = normalize(samples, axis=(1, 2))\n",
    "predictions = predictor.predict(np.expand_dims(samples, 3))[\"predictions\"]  # add channel dim\n",
    "\n",
    "# softmax to logit\n",
    "predictions = np.array(predictions, dtype=np.float32)\n",
    "predictions = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predictions: \", predictions.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Clean up \n",
    "\n",
    "If you do not plan to use the endpoint, you should delete it to free up some computation \n",
    "resource. If you use local, you will need to manually delete the docker container bounded\n",
    "at port 8080 (the port that listens to the incoming request).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not local_mode:\n",
    "    predictor.delete_endpoint()\n",
    "else:\n",
    "    os.system(\"docker container ls | grep 8080 | awk '{print $1}' | xargs docker container rm -f\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
